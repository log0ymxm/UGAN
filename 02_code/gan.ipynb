{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative adversarial networks (GANs) are a powerful approach for\n",
    "probabilistic modeling (I. Goodfellow et al., 2014; I. Goodfellow, 2016).\n",
    "They posit a deep generative model and they enable fast and accurate\n",
    "inferences.\n",
    "\n",
    "This mimics Edward tutorial on GAN at:\n",
    "http://edwardlib.org/tutorials/gan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Uniform\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "# Some description about Ganesha images??\n",
    "\n",
    "![Ganesha](https://s-media-cache-ak0.pinimg.com/736x/d3/bf/35/d3bf350947dfb8cc5c845c62f7be5d29.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data set\n",
    "data_width = 50\n",
    "data_height = 50\n",
    "data_dim =   data_width * data_height\n",
    "\n",
    "data_file_path = 'ganesh_preproecessed_images.csv'\n",
    "X = np.array(pd.read_csv(data_file_path, header=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  gs = gridspec.GridSpec(4, 4)\n",
    "  gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "  for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.imshow(sample.reshape(data_width, data_height), cmap='Greys_r')\n",
    "\n",
    "  return fig\n",
    "\n",
    "\n",
    "ed.set_seed(42)\n",
    "\n",
    "M = 300  # batch size during training\n",
    "d = 100  # latent dimension\n",
    "\n",
    "DATA_DIR = \"data/mnist\"\n",
    "IMG_DIR = \"img\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  os.makedirs(DATA_DIR)\n",
    "if not os.path.exists(IMG_DIR):\n",
    "  os.makedirs(IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ph = tf.placeholder(tf.float32, [M, data_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "GANs posit generative models using an implicit mechanism. Given some\n",
    "random noise, the data is assumed to be generated by a deterministic\n",
    "function of that noise.\n",
    "\n",
    "Formally, the generative process is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\epsilon} &\\sim p(\\mathbf{\\epsilon}), \\\\\n",
    "\\mathbf{x} &= G(\\mathbf{\\epsilon}; \\theta),\n",
    "\\end{align*}\n",
    "\n",
    "where $G(\\cdot; \\theta)$ is a neural network that takes the samples\n",
    "$\\mathbf{\\epsilon}$ as input. The distribution\n",
    "$p(\\mathbf{\\epsilon})$ is interpreted as random noise injected to\n",
    "produce stochasticity in a physical system; it is typically a fixed\n",
    "uniform or normal distribution with some latent dimensionality.\n",
    "\n",
    "In Edward, we build the model as follows, using TensorFlow Slim to\n",
    "specify the neural network. It defines a 2-layer fully connected neural\n",
    "network and outputs a vector of length $28\\times28$ with values in\n",
    "$[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_network(eps):\n",
    "  h1 = slim.fully_connected(eps, 128, activation_fn=tf.nn.relu)\n",
    "#   x = slim.fully_connected(h1, 784, activation_fn=tf.sigmoid)\n",
    "  x = slim.fully_connected(h1, data_dim, activation_fn=tf.sigmoid) #modified\n",
    "  return x\n",
    "\n",
    "with tf.variable_scope(\"Gen\"):\n",
    "  eps = Uniform(tf.zeros([M, d]) - 1.0, tf.ones([M, d]))\n",
    "  x = generative_network(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to estimate parameters of the generative network such\n",
    "that the model best captures the data. (Note in GANs, we are\n",
    "interested only in parameter estimation and not inference about any\n",
    "latent variables.)\n",
    "\n",
    "Unfortunately, probability models described above do not admit a tractable\n",
    "likelihood. This poses a problem for most inference algorithms, as\n",
    "they usually require taking the model's density.  Thus we are\n",
    "motivated to use \"likelihood-free\" algorithms\n",
    "(Marin, Pudlo, Robert, & Ryder, 2012), a class of methods which assume one\n",
    "can only sample from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A key idea in likelihood-free methods is to learn by\n",
    "comparison (e.g., Rubin (1984; Gretton, Borgwardt, Rasch, Schölkopf, & Smola, 2012)): by\n",
    "analyzing the discrepancy between samples from the model and samples\n",
    "from the true data distribution, we have information on where the\n",
    "model can be improved in order to generate better samples.\n",
    "\n",
    "In GANs, a neural network $D(\\cdot;\\phi)$ makes this comparison,\n",
    "known as the discriminator.\n",
    "$D(\\cdot;\\phi)$ takes data $\\mathbf{x}$ as input (either\n",
    "generations from the model or data points from the data set), and it\n",
    "calculates the probability that $\\mathbf{x}$ came from the true data.\n",
    "\n",
    "In Edward, we use the following discriminative network. It is simply a\n",
    "feedforward network with one ReLU hidden layer. It returns the\n",
    "probability in the logit (unconstrained) scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminative_network(x):\n",
    "  \"\"\"Outputs probability in logits.\"\"\"\n",
    "  h1 = slim.fully_connected(x, 128, activation_fn=tf.nn.relu)\n",
    "  logit = slim.fully_connected(h1, 1, activation_fn=None)\n",
    "  return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p^*(\\mathbf{x})$ represent the true data distribution.\n",
    "The optimization problem used in GANs is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\min_\\theta \\max_\\phi~\n",
    "\\mathbb{E}_{p^*(\\mathbf{x})} [ \\log D(\\mathbf{x}; \\phi) ]\n",
    "+ \\mathbb{E}_{p(\\mathbf{x}; \\theta)} [ \\log (1 - D(\\mathbf{x}; \\phi)) ].\n",
    "\\end{equation*}\n",
    "\n",
    "This optimization problem is bilevel: it requires a minima solution\n",
    "with respect to generative parameters and a maxima solution with\n",
    "respect to discriminative parameters.\n",
    "In practice, the algorithm proceeds by iterating gradient updates on\n",
    "each. An\n",
    "additional heuristic also modifies the objective function for the\n",
    "generative model in order to avoid saturation of gradients\n",
    "(I. J. Goodfellow, 2014).\n",
    "\n",
    "Many sources of intuition exist behind GAN-style training. One, which\n",
    "is the original motivation, is based on idea that the two neural\n",
    "networks are playing a game. The discriminator tries to best\n",
    "distinguish samples away from the generator. The generator tries\n",
    "to produce samples that are indistinguishable by the discriminator.\n",
    "The goal of training is to reach a Nash equilibrium.\n",
    "\n",
    "Another source is the idea of casting unsupervised learning as\n",
    "supervised learning\n",
    "(M. U. Gutmann, Dutta, Kaski, & Corander, 2014; M. Gutmann & Hyvärinen, 2010).\n",
    "This allows one to leverage the power of classification—a problem that\n",
    "in recent years is (relatively speaking) very easy.\n",
    "\n",
    "A third comes from classical statistics, where the discriminator is\n",
    "interpreted as a proxy of the density ratio between the true data\n",
    "distribution and the model\n",
    " (Mohamed & Lakshminarayanan, 2016; Sugiyama, Suzuki, & Kanamori, 2012). By augmenting an\n",
    "original problem that may require the model's density with a\n",
    "discriminator (such as maximum likelihood), one can recover the\n",
    "original problem when the discriminator is optimal. Furthermore, this\n",
    "approximation is very fast, and it justifies GANs from the perspective\n",
    "of approximate inference.\n",
    "\n",
    "In Edward, the GAN algorithm (`GANInference`) simply takes the\n",
    "implicit density model on `x` as input, binded to its\n",
    "realizations `x_ph`. In addition, a parameterized function\n",
    "`discriminator` is provided to distinguish their\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = ed.GANInference(\n",
    "    data={x: x_ph}, discriminator=discriminative_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use ADAM as optimizers for both the generator and discriminator.\n",
    "We'll run the algorithm for 15,000 iterations and print progress every\n",
    "1,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "optimizer_d = tf.train.AdamOptimizer()\n",
    "\n",
    "inference = ed.GANInference(\n",
    "    data={x: x_ph}, discriminator=discriminative_network)\n",
    "inference.initialize(\n",
    "    optimizer=optimizer, optimizer_d=optimizer_d,\n",
    "    n_iter=10000, n_print=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now form the main loop which trains the GAN. At each iteration, it\n",
    "takes a minibatch and updates the parameters according to the\n",
    "algorithm. At every 1000 iterations, it will print progress and also\n",
    "saves a figure of generated samples from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ganesh_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2eb3dbdd9247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#   x_batch, _ = mnist.train.next_batch(M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mganesh_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ganesh_data' is not defined"
     ]
    }
   ],
   "source": [
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "idx = np.random.randint(M, size=16)\n",
    "i = 0\n",
    "for t in range(inference.n_iter):\n",
    "  if t % inference.n_print == 0:\n",
    "    samples = sess.run(x)\n",
    "    samples = samples[idx, ]\n",
    "\n",
    "    fig = plot(samples)\n",
    "    plt.savefig(os.path.join(IMG_DIR, '{}.png').format(\n",
    "        str(i).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    i += 1\n",
    "\n",
    "#   x_batch, _ = mnist.train.next_batch(M)\n",
    "  x_batch = X[np.random.choice(np.arange(ganesh_data.shape[0]), size=M)]\n",
    "  info_dict = inference.update(feed_dict={x_ph: x_batch})\n",
    "  inference.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining convergence of the GAN objective can be meaningless in\n",
    "practice. The algorithm is usually run until some other criterion is\n",
    "satisfied, such as if the samples look visually okay, or if the GAN\n",
    "can capture meaningful parts of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism\n",
    "\n",
    "Evaluation of GANs remains an open problem---both in criticizing their\n",
    "fit to data and in assessing convergence.\n",
    "Recent advances have considered alternative objectives and\n",
    "heuristics to stabilize training (see also Soumith Chintala's\n",
    "[GAN hacks repo](https://github.com/soumith/ganhacks))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
